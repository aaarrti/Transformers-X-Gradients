{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T12:48:55.214585Z",
     "start_time": "2023-04-30T12:48:55.197331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from transformers_gradients import text_classification, html_heatmap, normalize_sum_to_1\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T12:48:58.871040Z",
     "start_time": "2023-04-30T12:48:55.215105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:48:58,892:[connectionpool.py:1003->_new_conn()]:DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-04-30 14:48:59,045:[connectionpool.py:456->_make_request()]:DEBUG: https://huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "2023-04-30 14:49:00,346:[connectionpool.py:1003->_new_conn()]:DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-04-30 14:49:00,511:[connectionpool.py:456->_make_request()]:DEBUG: https://huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T12:49:00.550015Z",
     "start_time": "2023-04-30T12:48:58.871369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "x = [\"Like four times a year I rediscover Bj√∂rk and listen to her full discography\"]\n",
    "y = [1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T12:49:00.580267Z",
     "start_time": "2023-04-30T12:49:00.543982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e099929bedd446d9854a2dd678bbe4bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:49:00.891263: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m a_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     f(model, x, y, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m tqdm(\n\u001B[1;32m      4\u001B[0m         [\n\u001B[1;32m      5\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mgradient_norm,\n\u001B[1;32m      6\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mgradient_x_input,\n\u001B[1;32m      7\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mintegrated_gradients,\n\u001B[1;32m      8\u001B[0m             partial(\n\u001B[1;32m      9\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     10\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     11\u001B[0m             ),\n\u001B[1;32m     12\u001B[0m             partial(\n\u001B[1;32m     13\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     14\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     15\u001B[0m             ),\n\u001B[1;32m     16\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     17\u001B[0m             partial(\n\u001B[1;32m     18\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     19\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     20\u001B[0m             ),\n\u001B[1;32m     21\u001B[0m             partial(\n\u001B[1;32m     22\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     23\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     24\u001B[0m             ),\n\u001B[1;32m     25\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     26\u001B[0m             partial(\n\u001B[1;32m     27\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     28\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     29\u001B[0m             ),\n\u001B[1;32m     30\u001B[0m             partial(\n\u001B[1;32m     31\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     32\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     33\u001B[0m             ),\n\u001B[1;32m     34\u001B[0m             \u001B[38;5;66;03m# The last one is very slow for 1 sample, but it get bitter as batch size is maximized.\u001B[39;00m\n\u001B[1;32m     35\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     36\u001B[0m         ]\n\u001B[1;32m     37\u001B[0m     )\n\u001B[1;32m     38\u001B[0m ]\n",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m a_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m tqdm(\n\u001B[1;32m      4\u001B[0m         [\n\u001B[1;32m      5\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mgradient_norm,\n\u001B[1;32m      6\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mgradient_x_input,\n\u001B[1;32m      7\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mintegrated_gradients,\n\u001B[1;32m      8\u001B[0m             partial(\n\u001B[1;32m      9\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     10\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     11\u001B[0m             ),\n\u001B[1;32m     12\u001B[0m             partial(\n\u001B[1;32m     13\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     14\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     15\u001B[0m             ),\n\u001B[1;32m     16\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39msmooth_grad,\n\u001B[1;32m     17\u001B[0m             partial(\n\u001B[1;32m     18\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     19\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     20\u001B[0m             ),\n\u001B[1;32m     21\u001B[0m             partial(\n\u001B[1;32m     22\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     23\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     24\u001B[0m             ),\n\u001B[1;32m     25\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mnoise_grad,\n\u001B[1;32m     26\u001B[0m             partial(\n\u001B[1;32m     27\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     28\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     29\u001B[0m             ),\n\u001B[1;32m     30\u001B[0m             partial(\n\u001B[1;32m     31\u001B[0m                 text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     32\u001B[0m                 config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(explain_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradXInput\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     33\u001B[0m             ),\n\u001B[1;32m     34\u001B[0m             \u001B[38;5;66;03m# The last one is very slow for 1 sample, but it get bitter as batch size is maximized.\u001B[39;00m\n\u001B[1;32m     35\u001B[0m             text_classification\u001B[38;5;241m.\u001B[39mfusion_grad,\n\u001B[1;32m     36\u001B[0m         ]\n\u001B[1;32m     37\u001B[0m     )\n\u001B[1;32m     38\u001B[0m ]\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/api.py:472\u001B[0m, in \u001B[0;36mtext_classification.fusion_grad\u001B[0;34m(model, x_batch, y_batch, tokenizer, attention_mask, config)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;124;03mFusionGrad is a fusion of NoiseGrad and SmoothGrad methods.\u001B[39;00m\n\u001B[1;32m    436\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    466\u001B[0m \n\u001B[1;32m    467\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers_gradients\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtasks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_classification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m    469\u001B[0m     fusion_grad,\n\u001B[1;32m    470\u001B[0m )\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfusion_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:84\u001B[0m, in \u001B[0;36mplain_text_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, tokenizer, **kwargs)\u001B[0m\n\u001B[1;32m     82\u001B[0m input_ids, attention_mask \u001B[38;5;241m=\u001B[39m encode_inputs(tokenizer, x_batch)\n\u001B[1;32m     83\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_input_embeddings()(input_ids)\n\u001B[0;32m---> 84\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m    \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m    \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers_gradients\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mreturn_raw_scores:\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:54\u001B[0m, in \u001B[0;36mtensor_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m value_or_default(\n\u001B[1;32m     51\u001B[0m     attention_mask, partial(default_attention_mask, x_batch)\n\u001B[1;32m     52\u001B[0m )\n\u001B[1;32m     53\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m as_tensor(attention_mask)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:313\u001B[0m, in \u001B[0;36mfusion_grad\u001B[0;34m(model, x_batch, y_batch, attention_mask, config)\u001B[0m\n\u001B[1;32m    306\u001B[0m sg_explain_fn \u001B[38;5;241m=\u001B[39m partial(smooth_grad, config\u001B[38;5;241m=\u001B[39msg_config)\n\u001B[1;32m    307\u001B[0m ng_config \u001B[38;5;241m=\u001B[39m NoiseGradConfig(\n\u001B[1;32m    308\u001B[0m     n\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mn,\n\u001B[1;32m    309\u001B[0m     mean\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmean,\n\u001B[1;32m    310\u001B[0m     explain_fn\u001B[38;5;241m=\u001B[39msg_explain_fn,  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     noise_fn\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnoise_fn,\n\u001B[1;32m    312\u001B[0m )\n\u001B[0;32m--> 313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnoise_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mng_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:71\u001B[0m, in \u001B[0;36mplain_text_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, tokenizer, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\n\u001B[1;32m     62\u001B[0m     model: TFPreTrainedModel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     69\u001B[0m ):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_batch[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 71\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMust provide tokenizer for plain-text inputs.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:54\u001B[0m, in \u001B[0;36mtensor_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m value_or_default(\n\u001B[1;32m     51\u001B[0m     attention_mask, partial(default_attention_mask, x_batch)\n\u001B[1;32m     52\u001B[0m )\n\u001B[1;32m     53\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m as_tensor(attention_mask)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:277\u001B[0m, in \u001B[0;36mnoise_grad\u001B[0;34m(model, x_batch, y_batch, attention_mask, config)\u001B[0m\n\u001B[1;32m    271\u001B[0m     noisy_weights \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    272\u001B[0m         noise_fn,\n\u001B[1;32m    273\u001B[0m         original_weights,\n\u001B[1;32m    274\u001B[0m     )\n\u001B[1;32m    275\u001B[0m     model\u001B[38;5;241m.\u001B[39mset_weights(noisy_weights)\n\u001B[0;32m--> 277\u001B[0m     explanation \u001B[38;5;241m=\u001B[39m \u001B[43mexplain_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m     explanations_array \u001B[38;5;241m=\u001B[39m explanations_array\u001B[38;5;241m.\u001B[39mwrite(n, explanation)\n\u001B[1;32m    280\u001B[0m scores \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_mean(explanations_array\u001B[38;5;241m.\u001B[39mstack(), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:71\u001B[0m, in \u001B[0;36mplain_text_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, tokenizer, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\n\u001B[1;32m     62\u001B[0m     model: TFPreTrainedModel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     69\u001B[0m ):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_batch[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 71\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMust provide tokenizer for plain-text inputs.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:54\u001B[0m, in \u001B[0;36mtensor_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m value_or_default(\n\u001B[1;32m     51\u001B[0m     attention_mask, partial(default_attention_mask, x_batch)\n\u001B[1;32m     52\u001B[0m )\n\u001B[1;32m     53\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m as_tensor(attention_mask)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:232\u001B[0m, in \u001B[0;36msmooth_grad\u001B[0;34m(model, x_batch, y_batch, attention_mask, config)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mrange(config\u001B[38;5;241m.\u001B[39mn):\n\u001B[1;32m    231\u001B[0m     noisy_x \u001B[38;5;241m=\u001B[39m noise_fn(x_batch)\n\u001B[0;32m--> 232\u001B[0m     explanation \u001B[38;5;241m=\u001B[39m \u001B[43mexplain_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoisy_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    233\u001B[0m     explanations_array \u001B[38;5;241m=\u001B[39m explanations_array\u001B[38;5;241m.\u001B[39mwrite(n, explanation)\n\u001B[1;32m    235\u001B[0m scores \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_mean(explanations_array\u001B[38;5;241m.\u001B[39mstack(), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:71\u001B[0m, in \u001B[0;36mplain_text_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, tokenizer, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\n\u001B[1;32m     62\u001B[0m     model: TFPreTrainedModel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     69\u001B[0m ):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_batch[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 71\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMust provide tokenizer for plain-text inputs.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:54\u001B[0m, in \u001B[0;36mtensor_inputs.<locals>.wrapper\u001B[0;34m(model, x_batch, y_batch, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m value_or_default(\n\u001B[1;32m     51\u001B[0m     attention_mask, partial(default_attention_mask, x_batch)\n\u001B[1;32m     52\u001B[0m )\n\u001B[1;32m     53\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m as_tensor(attention_mask)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/IdeaProjects/transformersXgradients/transformers_gradients/tasks/text_classification.py:189\u001B[0m, in \u001B[0;36mintegrated_gradients\u001B[0;34m(model, x_batch, y_batch, attention_mask, num_steps, baseline_fn)\u001B[0m\n\u001B[1;32m    181\u001B[0m         logits \u001B[38;5;241m=\u001B[39m model(\n\u001B[1;32m    182\u001B[0m             \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    183\u001B[0m             inputs_embeds\u001B[38;5;241m=\u001B[39minterpolation_step,\n\u001B[1;32m    184\u001B[0m             training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    185\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    186\u001B[0m         )\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m    187\u001B[0m         logits \u001B[38;5;241m=\u001B[39m logits_for_labels(logits, y_batch)\n\u001B[0;32m--> 189\u001B[0m     grads \u001B[38;5;241m=\u001B[39m \u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     interpolated_grads \u001B[38;5;241m=\u001B[39m interpolated_grads\u001B[38;5;241m.\u001B[39mwrite(i, grads)\n\u001B[1;32m    192\u001B[0m interpolated_grads_tensor \u001B[38;5;241m=\u001B[39m interpolated_grads\u001B[38;5;241m.\u001B[39mstack()\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1063\u001B[0m, in \u001B[0;36mGradientTape.gradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1057\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1058\u001B[0m       composite_tensor_gradient\u001B[38;5;241m.\u001B[39mget_flat_tensors_for_gradients(\n\u001B[1;32m   1059\u001B[0m           output_gradients))\n\u001B[1;32m   1060\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x)\n\u001B[1;32m   1061\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m output_gradients]\n\u001B[0;32m-> 1063\u001B[0m flat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mimperative_grad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimperative_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1064\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_targets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1066\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_sources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflat_sources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munconnected_gradients\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_persistent:\n\u001B[1;32m   1072\u001B[0m   \u001B[38;5;66;03m# Keep track of watched variables before setting tape to None\u001B[39;00m\n\u001B[1;32m   1073\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_watched_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tape\u001B[38;5;241m.\u001B[39mwatched_variables()\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001B[0m, in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     65\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown value for unconnected_gradients: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m unconnected_gradients)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_TapeGradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:146\u001B[0m, in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    144\u001B[0m     gradient_name_scope \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m forward_pass_name_scope \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    145\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(gradient_name_scope):\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgrad_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmock_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mout_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    148\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m grad_fn(mock_op, \u001B[38;5;241m*\u001B[39mout_grads)\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:1338\u001B[0m, in \u001B[0;36m_SubGrad\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m   1336\u001B[0m   gx \u001B[38;5;241m=\u001B[39m grad\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1338\u001B[0m   gx \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39mreshape(\u001B[43mmath_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrx\u001B[49m\u001B[43m)\u001B[49m, sx)\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip_input_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01min\u001B[39;00m skip_input_indices:\n\u001B[1;32m   1340\u001B[0m   gy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1174\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[1;32m   1175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1176\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1178\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[1;32m   1179\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[1;32m   1180\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2364\u001B[0m, in \u001B[0;36mreduce_sum\u001B[0;34m(input_tensor, axis, keepdims, name)\u001B[0m\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmath.reduce_sum\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreduce_sum\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   2302\u001B[0m \u001B[38;5;129m@dispatch\u001B[39m\u001B[38;5;241m.\u001B[39madd_dispatch_support\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreduce_sum\u001B[39m(input_tensor, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   2304\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001B[39;00m\n\u001B[1;32m   2305\u001B[0m \n\u001B[1;32m   2306\u001B[0m \u001B[38;5;124;03m  This is the reduction operation for the elementwise `tf.math.add` op.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2361\u001B[0m \u001B[38;5;124;03m  @end_compatibility\u001B[39;00m\n\u001B[1;32m   2362\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2364\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce_sum_with_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[43m                              \u001B[49m\u001B[43m_ReductionDims\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2376\u001B[0m, in \u001B[0;36mreduce_sum_with_dims\u001B[0;34m(input_tensor, axis, keepdims, name, dims)\u001B[0m\n\u001B[1;32m   2368\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreduce_sum_with_dims\u001B[39m(input_tensor,\n\u001B[1;32m   2369\u001B[0m                          axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2370\u001B[0m                          keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   2371\u001B[0m                          name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2372\u001B[0m                          dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   2373\u001B[0m   keepdims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m keepdims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(keepdims)\n\u001B[1;32m   2374\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _may_reduce_to_scalar(\n\u001B[1;32m   2375\u001B[0m       keepdims, axis,\n\u001B[0;32m-> 2376\u001B[0m       \u001B[43mgen_math_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniconda3/envs/transformersXGradients/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:11408\u001B[0m, in \u001B[0;36m_sum\u001B[0;34m(input, axis, keep_dims, name)\u001B[0m\n\u001B[1;32m  11406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m  11407\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m> 11408\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  11409\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkeep_dims\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_dims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m  11410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m  11411\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "a_batch = [\n",
    "    f(model, x, y, tokenizer=tokenizer)[0]\n",
    "    for f in tqdm(\n",
    "        [\n",
    "            text_classification.gradient_norm,\n",
    "            text_classification.gradient_x_input,\n",
    "            text_classification.integrated_gradients,\n",
    "            partial(\n",
    "                text_classification.smooth_grad,\n",
    "                config=dict(explain_fn=\"GradNorm\"),\n",
    "            ),\n",
    "            partial(\n",
    "                text_classification.smooth_grad,\n",
    "                config=dict(explain_fn=\"GradXInput\"),\n",
    "            ),\n",
    "            text_classification.smooth_grad,\n",
    "            partial(\n",
    "                text_classification.noise_grad,\n",
    "                config=dict(explain_fn=\"GradNorm\"),\n",
    "            ),\n",
    "            partial(\n",
    "                text_classification.noise_grad,\n",
    "                config=dict(explain_fn=\"GradXInput\"),\n",
    "            ),\n",
    "            text_classification.noise_grad,\n",
    "            partial(\n",
    "                text_classification.fusion_grad,\n",
    "                config=dict(explain_fn=\"GradNorm\"),\n",
    "            ),\n",
    "            partial(\n",
    "                text_classification.fusion_grad,\n",
    "                config=dict(explain_fn=\"GradXInput\"),\n",
    "            ),\n",
    "            # The last one is very slow for 1 sample, but it get bitter as batch size is maximized.\n",
    "            text_classification.fusion_grad,\n",
    "        ]\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T12:53:30.735004Z",
     "start_time": "2023-04-30T12:49:00.580813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_batch = [(a, normalize_sum_to_1(b)) for a, b in a_batch]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "html_heatmap(\n",
    "    a_batch,\n",
    "    labels=[\n",
    "        \"Gradient Norm\",\n",
    "        \"Gradient X Input\",\n",
    "        \"Integrated Gradients\",\n",
    "        \"Smooth Grad + Gradient Norm\",\n",
    "        \"Noise Grad + Gradient Norm\",\n",
    "        \"FusionGrad + Gradient Norm\",\n",
    "        \"FusionGrad + Gradient X Input\",\n",
    "        \"FusionGrad + Gradient X Input\",\n",
    "        \"FusionGrad + Gradient X Input\",\n",
    "        \"FusionGrad + Integrated Gradients\",\n",
    "        \"FusionGrad + Integrated Gradients\",\n",
    "        \"FusionGrad + Integrated Gradients\",\n",
    "    ],\n",
    "    config=dict(color_mapping_strategy=\"global\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
